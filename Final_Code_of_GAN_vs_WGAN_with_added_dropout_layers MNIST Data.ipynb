{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Code of GAN vs. WGAN with added dropout layers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRC5y_IGFZGe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ersQH-lFiu1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import argparse\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "def run(input,model_parameters):\n",
        "  if input:\n",
        "    model_param = take_input()\n",
        "  else:\n",
        "    model_param = model_parameters\n",
        "  \n",
        "  make_dir_if_not_exists(model_param.get(\"output\"))\n",
        "  #if not os.path.exists(model_param.get(\"output\")) or not os.path.isdir(model_param.get(\"output\")):\n",
        "        #raise OSError(\"Output path does not exist or is not a directory.\")\n",
        "  \n",
        "  model_param['output'] = os.path.normpath(model_param.get(\"output\"))\n",
        "  \n",
        "  if model_param.get(\"model\") == \"GAN\":\n",
        "    model = GAN(model_param)\n",
        "  else:\n",
        "    model = WGAN_LP(model_param)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  # randomly generate 100 samples\n",
        "  if model_param.get(\"visualize\"):\n",
        "      vis_seed = tf.random.uniform([100, model.noise_dim])\n",
        "      vis_gen = model.G(vis_seed, training=False)\n",
        "      if model_param.get(\"dataset\") == \"MNIST\":\n",
        "          plt.figure(figsize=(4.8, 4.8))\n",
        "      else:\n",
        "          plt.figure(figsize=(5.4, 5.4))\n",
        "      for i in range(vis_gen.shape[0]):\n",
        "          x_pos = i % 10\n",
        "          y_pos = int(i / 10)\n",
        "          if model_param.get(\"dataset\") == \"MNIST\":\n",
        "              plt.figimage(vis_gen[i, :, :, 0] * 127.5 + 127.5,\n",
        "                            10 + x_pos * (28 + 5), 10 + y_pos * (28 + 5), cmap='gray')\n",
        "          else:\n",
        "              plt.figimage((vis_gen[i, :, :] + 1) / 2,\n",
        "                            10 + x_pos * (32 + 5), 10 + y_pos * (32 + 5))\n",
        "          plt.axis('off')\n",
        "      make_dir_if_not_exists(os.path.join(model_param.get(\"output\"),\n",
        "                                \"{}_{}_Example.png\".format(model_param.get(\"model\"), model_param.get(\"dataset\"))))    \n",
        "      plt.savefig(os.path.join(model_param.get(\"output\"),\n",
        "                                \"{}_{}_Example.png\".format(model_param.get(\"model\"), model_param.get(\"dataset\"))))\n",
        "\n",
        "      # plot median value of the objective functions\n",
        "      plt.figure()\n",
        "      plt.title(\"Objective Functions of {} (Dataset: {})\".format(model_param.get(\"model\"), model_param.get(\"dataset\")))\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Median Value\")\n",
        "      plt.plot(range(1, 1 + model_param.get(\"total_epoch\")), np.median(model.d_obj, axis=[-0, -1]))\n",
        "      plt.plot(range(1, 1 + model_param.get(\"total_epoch\")), np.median(model.g_obj, axis=[-0]))\n",
        "      plt.legend(['Discriminator', 'Generator'])\n",
        "      plt.savefig(os.path.join(model_param.get(\"output\"),\n",
        "                                \"{}_{}_Objective.png\".format(model_param.get(\"model\"), model_param.get(\"dataset\"))))\n",
        "  if model_param.get(\"gif\"):\n",
        "      model.make_gif(model_param)\n",
        "\n",
        "\n",
        "class GAN:\n",
        "    def __init__(self, param):\n",
        "        # load data\n",
        "        self.dataset = param.get(\"dataset\", -1)\n",
        "\n",
        "        # choose CNN setup for the dataset\n",
        "        if self.dataset == \"MNIST\":\n",
        "            (self.x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "            self.init_dim = 7\n",
        "            self.strides = (1, 2, 2)\n",
        "            self.data_shape = self.x_train.shape + (1,)\n",
        "        elif self.dataset == \"CIFAR10\":\n",
        "            (self.x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "            self.init_dim = 4\n",
        "            self.strides = (2, 2, 2)\n",
        "            self.data_shape = self.x_train.shape\n",
        "        else:\n",
        "            raise ValueError('Dataset not supported.')\n",
        "\n",
        "        # get basic inputs\n",
        "        self.batch_size = param.get(\"batch_size\", 128)\n",
        "        self.noise_dim = param.get(\"noise_dim\", 128)\n",
        "        self.total_epoch = param.get(\"total_epoch\", 100)\n",
        "        self.critic_step = param.get(\"critic_step\", 1)\n",
        "        self.visualize = param.get(\"visualize\", True)\n",
        "        self.out_path = param.get(\"output\", \"/content/gdrive/My Drive/GANvWGAN/fileoutputs\")\n",
        "\n",
        "        # storage for the objectives\n",
        "        self.batch_num = int(self.data_shape[0] / self.batch_size) + (self.data_shape[0] % self.batch_size != 0)\n",
        "        self.d_obj = np.zeros([self.batch_num, self.total_epoch, self.critic_step])\n",
        "        self.g_obj = np.zeros([self.batch_num, self.total_epoch])\n",
        "\n",
        "        # normalize dataset\n",
        "        self.x_train = self.x_train.reshape(self.data_shape).astype('float32')\n",
        "        self.x_train = (self.x_train - 127.5) / 127.5  # Normalize RGB to [-1, 1]\n",
        "        self.x_train = \\\n",
        "            tf.data.Dataset.from_tensor_slices(self.x_train).shuffle(self.data_shape[0]).batch(self.batch_size)\n",
        "\n",
        "        # setup optimizers\n",
        "        self.G_optimizer = tf.keras.optimizers.Adam(learning_rate=param.get(\"learning_rate_g\", 5e-5),\n",
        "                                                    beta_1=param.get(\"beta_1_g\", 0.5),\n",
        "                                                    beta_2=param.get(\"beta_2_g\", 0.999),\n",
        "                                                    epsilon=param.get(\"epsilon_g\", 1e-7),\n",
        "                                                    amsgrad=param.get(\"amsgrad_g\", False))\n",
        "        self.D_optimizer = tf.keras.optimizers.Adam(learning_rate=param.get(\"learning_rate_d\", 1e-4),\n",
        "                                                    beta_1=param.get(\"beta_1_d\", 0.5),\n",
        "                                                    beta_2=param.get(\"beta_2_d\", 0.999),\n",
        "                                                    epsilon=param.get(\"epsilon_d\", 1e-7),\n",
        "                                                    amsgrad=param.get(\"amsgrad_d\", False))\n",
        "        # setup models\n",
        "        self.G = self.set_generator()\n",
        "        self.D = self.set_discriminator()\n",
        "\n",
        "    def set_generator(self):\n",
        "        g = tf.keras.Sequential()\n",
        "        g.add(layers.Dense(self.init_dim * self.init_dim * 256, use_bias=False, input_shape=(self.noise_dim,)))\n",
        "        g.add(layers.BatchNormalization())\n",
        "        g.add(layers.LeakyReLU())\n",
        "        g.add(layers.Reshape((self.init_dim, self.init_dim, 256)))\n",
        "\n",
        "        g.add(layers.Conv2DTranspose(128, 5, strides=self.strides[0], padding='same', use_bias=False))\n",
        "        g.add(layers.BatchNormalization())\n",
        "        g.add(layers.LeakyReLU())\n",
        "\n",
        "        g.add(layers.Conv2DTranspose(64, 5, strides=self.strides[1], padding='same', use_bias=False))\n",
        "        g.add(layers.BatchNormalization())\n",
        "        g.add(layers.LeakyReLU())\n",
        "\n",
        "        g.add(layers.Conv2DTranspose(32, 5, strides=self.strides[2], padding='same', use_bias=False))\n",
        "        g.add(layers.BatchNormalization())\n",
        "        g.add(layers.LeakyReLU())\n",
        "\n",
        "        g.add(layers.Conv2DTranspose(self.data_shape[3], 5, padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "        return g\n",
        "\n",
        "    def set_discriminator(self):\n",
        "        d = tf.keras.Sequential()\n",
        "        d.add(layers.Conv2D(32, kernel_size=5, strides=2, padding='same',\n",
        "                            input_shape=self.data_shape[1:],\n",
        "                            kernel_initializer=\"glorot_uniform\"))\n",
        "        d.add(layers.LeakyReLU())\n",
        "        d.add(layers.Dropout(0.2))\n",
        "\n",
        "        d.add(layers.Conv2D(64, kernel_size=5, strides=2, padding='same'))\n",
        "        d.add(layers.BatchNormalization())\n",
        "        d.add(layers.LeakyReLU())\n",
        "        d.add(layers.Dropout(0.2))\n",
        "\n",
        "        d.add(layers.Conv2D(128, kernel_size=5, strides=2, padding='same'))\n",
        "        d.add(layers.BatchNormalization())\n",
        "        d.add(layers.LeakyReLU())\n",
        "        d.add(layers.Dropout(0.5))\n",
        "\n",
        "        d.add(layers.Conv2D(256, kernel_size=5, strides=2, padding='same'))\n",
        "        d.add(layers.BatchNormalization())\n",
        "        d.add(layers.LeakyReLU())\n",
        "\n",
        "        d.add(layers.Flatten())\n",
        "        d.add(layers.Dense(2, activation='softmax'))\n",
        "        d.add(layers.Lambda(lambda x: x[:, 0]))\n",
        "\n",
        "        return d\n",
        "\n",
        "    @tf.function\n",
        "    def train_discriminator(self, x_batch):\n",
        "        with tf.GradientTape() as D_tape:\n",
        "          x_gen = self.G(tf.random.uniform([x_batch.shape[0], self.noise_dim]), training=True)\n",
        "          y_real = self.D(x_batch, training=True)\n",
        "          y_gen = self.D(x_gen, training=True)\n",
        "\n",
        "          # compute the objective\n",
        "          loss_real = tf.math.log(tf.clip_by_value(y_real, 1e-10, 1.0))\n",
        "          loss_gen = tf.math.log(tf.clip_by_value(tf.math.add(1.0, tf.math.negative(y_gen)), 1e-10, 1.0))\n",
        "          d_obj = -tf.math.reduce_mean(loss_real) - tf.math.reduce_mean(loss_gen)\n",
        "          # update the discriminator\n",
        "          d_grad = D_tape.gradient(d_obj, self.D.trainable_variables)\n",
        "          self.D_optimizer.apply_gradients(zip(d_grad, self.D.trainable_variables))\n",
        "            \n",
        "        return d_obj\n",
        "\n",
        "    @tf.function\n",
        "    def train_generator(self, x_batch_size):\n",
        "        with tf.GradientTape() as G_tape:\n",
        "          x_gen = self.G(tf.random.uniform([x_batch_size, self.noise_dim]), training=True)\n",
        "          y_gen = self.D(x_gen, training=True)\n",
        "\n",
        "          # compute the objective\n",
        "          #g_obj = -tf.math.reduce_mean(tf.math.log(tf.clip_by_value(y_gen, 1e-10, 1.0)))\n",
        "          g_obj = tf.math.reduce_mean(tf.math.log(tf.clip_by_value(tf.math.add(1.0, tf.math.negative(y_gen)), 1e-10, 1.0)))\n",
        "          # update the generator\n",
        "          g_grad = G_tape.gradient(g_obj, self.G.trainable_variables)\n",
        "          self.G_optimizer.apply_gradients(zip(g_grad, self.G.trainable_variables))\n",
        "\n",
        "        return g_obj\n",
        "\n",
        "    def train(self):\n",
        "        vis_seed = None\n",
        "        if self.visualize:\n",
        "            # Seed for checking training progress\n",
        "            vis_seed = tf.random.uniform([16, self.noise_dim])\n",
        "\n",
        "        # Record current time and start training\n",
        "        print(\"Training...\")\n",
        "        ts_start = tf.timestamp()\n",
        "        for t in range(self.total_epoch):\n",
        "            batch_id = 0\n",
        "            for b in self.x_train:\n",
        "                for k in range(self.critic_step):\n",
        "                    self.d_obj[batch_id, t, k] = self.train_discriminator(b)\n",
        "                #print(self.train_generator(self.batch_size))    \n",
        "                self.g_obj[batch_id, t] = self.train_generator(b.shape[0])\n",
        "                batch_id += 1\n",
        "            \n",
        "            checkpoint_dir = './training_checkpoints/'\n",
        "            checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "            checkpoint = tf.train.Checkpoint(generator_optimizer=self.G_optimizer,\n",
        "                                            discriminator_optimizer=self.D_optimizer,\n",
        "                                            generator=self.G,\n",
        "                                            discriminator=self.D)\n",
        "\n",
        "            # Save the model every epoch\n",
        "            if (t + 1) % 1 == 0:\n",
        "              checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "            # Print time\n",
        "            print(\"Time used for epoch {} are {:0.2f} seconds.\".format(t + 1, tf.timestamp() - ts_start))\n",
        "\n",
        "            # Check current generator\n",
        "            if self.visualize:\n",
        "                vis_gen = self.G(vis_seed, training=False)\n",
        "                fig = plt.figure(figsize=(4, 4))\n",
        "                plt.suptitle('Epoch: {:03d}'.format(t + 1))\n",
        "                for i in range(vis_gen.shape[0]):\n",
        "                    plt.subplot(4, 4, i + 1)\n",
        "                    if self.data_shape[3] == 1:\n",
        "                        plt.imshow(vis_gen[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "                    else:\n",
        "                        plt.imshow((vis_gen[i, :, :] + 1) / 2)\n",
        "                    plt.axis('off')\n",
        "                make_dir_if_not_exists(os.path.join(self.out_path, \"GAN_{}_Epoch_{:03d}.png\".format(self.dataset, t + 1)))\n",
        "                plt.savefig(os.path.join(self.out_path, \"GAN_{}_Epoch_{:03d}.png\".format(self.dataset, t + 1)))\n",
        "                plt.clf()\n",
        "                plt.close(fig)\n",
        "        print(\"Done! {:0.2f} seconds have passed.\".format(tf.timestamp() - ts_start))\n",
        "\n",
        "    def restore_ckpt(self):\n",
        "      checkpoint_dir = checkpoint_dir = self.out_path + '/training_checkpoints/'\n",
        "      checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "    def display_image(self,epoch_no,example=False):\n",
        "      if example:\n",
        "        return PIL.Image.open(os.path.join(self.out_path,\"GAN_{}_Example.png\".format(self.dataset)))\n",
        "      else:\n",
        "        return PIL.Image.open(os.path.join(self.out_path, \"GAN_{}_Epoch_{:03d}.png\".format(self.dataset,epoch_no)))\n",
        "    \n",
        "    def make_gif(self,param):\n",
        "      anim_file = \"{}_{}_totalepoch{}.gif\".format(model_param.get(\"dataset\"),model_param.get(\"model\"),model_param.get(\"total_epoch\"))\n",
        "\n",
        "      os.chdir(model_param.get(\"output\"))\n",
        "      filenames = glob.glob('GAN_{}_Epoch_*.png'.format(model_param.get(\"dataset\")))\n",
        "      filenames = sorted(filenames)\n",
        "      image=[]\n",
        "      for i,filename in enumerate(filenames):\n",
        "        image.append(imageio.imread(filename))\n",
        "        imageio.mimsave(\"./\"+anim_file,iter(image))\n",
        "          \n",
        "      import IPython\n",
        "      if IPython.version_info > (6,2,0,''):\n",
        "        display.Image(filename=anim_file)\n",
        "\n",
        "class WGAN_LP:\n",
        "    def __init__(self, param):\n",
        "        # load data\n",
        "        self.dataset = param.get(\"dataset\", -1)\n",
        "\n",
        "        # choose CNN setup for each dataset\n",
        "        if self.dataset == \"MNIST\":\n",
        "            (self.x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "            self.init_dim = 7\n",
        "            self.strides = (2, 2, 1)\n",
        "            self.data_shape = self.x_train.shape + (1,)\n",
        "        elif self.dataset == \"CIFAR10\":\n",
        "            (self.x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "            self.init_dim = 2\n",
        "            self.strides = (2, 2, 2)\n",
        "            self.data_shape = self.x_train.shape\n",
        "        else:\n",
        "            raise ValueError('Dataset not supported.')\n",
        "\n",
        "        # get basic inputs\n",
        "        self.batch_size = param.get(\"batch_size\", 128)\n",
        "        self.noise_dim = param.get(\"noise_dim\", 128)\n",
        "        self.total_epoch = param.get(\"total_epoch\", 100)\n",
        "        self.critic_step = param.get(\"critic_step\", 1)\n",
        "        self.visualize = param.get(\"visualize\", True)\n",
        "        self.out_path = param.get(\"output\", \"/content/gdrive/My Drive/GANvWGAN/fileoutputs\")\n",
        "\n",
        "\n",
        "        # storage for the objectives\n",
        "        self.batch_num = int(self.data_shape[0] / self.batch_size) + (self.data_shape[0] % self.batch_size != 0)\n",
        "        self.d_obj = np.zeros([self.batch_num, self.total_epoch, self.critic_step])\n",
        "        self.g_obj = np.zeros([self.batch_num, self.total_epoch])\n",
        "\n",
        "        # set regularization parameters\n",
        "        self.grad_penalty = param.get(\"grad_penalty\", 10.0)\n",
        "        self.perturb_factor = param.get(\"perturb_factor\", 1.0)\n",
        "\n",
        "        # normalize dataset\n",
        "        self.x_train = self.x_train.reshape(self.data_shape).astype('float32')\n",
        "        self.x_train = (self.x_train - 127.5) / 127.5  # Normalize RGB to [-1, 1]\n",
        "        self.x_train = \\\n",
        "            tf.data.Dataset.from_tensor_slices(self.x_train).shuffle(self.data_shape[0]).batch(self.batch_size)\n",
        "\n",
        "        # setup optimizers\n",
        "        self.D_optimizer = tf.keras.optimizers.Adam(learning_rate=param.get(\"learning_rate_d\", 1e-4),\n",
        "                                                    beta_1=param.get(\"beta_1_d\", 0.5),\n",
        "                                                    beta_2=param.get(\"beta_2_d\", 0.999),\n",
        "                                                    epsilon=param.get(\"epsilon_d\", 1e-7),\n",
        "                                                    amsgrad=param.get(\"amsgrad_d\", False))\n",
        "        self.G_optimizer = tf.keras.optimizers.Adam(learning_rate=param.get(\"learning_rate_g\", 5e-5),\n",
        "                                                    beta_1=param.get(\"beta_1_g\", 0.2),\n",
        "                                                    beta_2=param.get(\"beta_2_g\", 0.999),\n",
        "                                                    epsilon=param.get(\"epsilon_g\", 1e-7),\n",
        "                                                    amsgrad=param.get(\"amsgrad_g\", False))\n",
        "\n",
        "        # setup models\n",
        "        self.G = self.set_generator()\n",
        "        self.D = self.set_discriminator()\n",
        "\n",
        "    def set_generator(self):\n",
        "        g = tf.keras.Sequential()\n",
        "        g.add(layers.Dense(self.init_dim * self.init_dim * 256, use_bias=False, input_shape=(self.noise_dim,)))\n",
        "        g.add(layers.BatchNormalization())\n",
        "        g.add(layers.LeakyReLU())\n",
        "        g.add(layers.Reshape((self.init_dim, self.init_dim, 256)))\n",
        "\n",
        "        g.add(layers.Conv2DTranspose(128, 5, strides=self.strides[0], padding='same', use_bias=False))\n",
        "        g.add(layers.BatchNormalization())\n",
        "        g.add(layers.LeakyReLU())\n",
        "\n",
        "        g.add(layers.Conv2DTranspose(64, 5, strides=self.strides[1], padding='same', use_bias=False))\n",
        "        g.add(layers.BatchNormalization())\n",
        "        g.add(layers.LeakyReLU())\n",
        "\n",
        "        g.add(layers.Conv2DTranspose(32, 5, strides=self.strides[2], padding='same', use_bias=False))\n",
        "        g.add(layers.BatchNormalization())\n",
        "        g.add(layers.LeakyReLU())\n",
        "\n",
        "        g.add(layers.Conv2DTranspose(self.data_shape[3], 5, strides=self.strides[2],\n",
        "                                     padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "        return g\n",
        "\n",
        "    def set_discriminator(self):\n",
        "        d = tf.keras.Sequential()\n",
        "        d.add(layers.Conv2D(32, kernel_size=5, strides=2, padding='same', input_shape=self.data_shape[1:]))\n",
        "        d.add(layers.LeakyReLU())\n",
        "        d.add(layers.Dropout(0.2))\n",
        "\n",
        "        d.add(layers.Conv2D(64, kernel_size=5, strides=2, padding='same'))\n",
        "        d.add(layers.LayerNormalization())\n",
        "        d.add(layers.LeakyReLU())\n",
        "        d.add(layers.Dropout(0.2))\n",
        "\n",
        "        d.add(layers.Conv2D(128, kernel_size=5, strides=2, padding='same'))\n",
        "        d.add(layers.LayerNormalization())\n",
        "        d.add(layers.LeakyReLU())\n",
        "\n",
        "        d.add(layers.Flatten())\n",
        "        d.add(layers.Dense(1))\n",
        "\n",
        "        return d\n",
        "\n",
        "    @tf.function\n",
        "    def lipschitz_penalty(self, x, x_hat):\n",
        "        # DRAGAN-like sampling scheme\n",
        "        x_join = tf.concat([x, x_hat], axis=0)\n",
        "        _, batch_var = tf.nn.moments(x_join, axes=[0, 1, 2, 3])\n",
        "        delta = tf.random.normal(x_join.shape, stddev=self.perturb_factor * tf.sqrt(batch_var))\n",
        "        x_tilde = x_join + delta\n",
        "\n",
        "        # compute gradient penalty\n",
        "        with tf.GradientTape() as D_tape:\n",
        "            D_tape.watch(x_tilde)\n",
        "            y_tilde = self.D(x_tilde)\n",
        "        d_grad = D_tape.gradient(y_tilde, x_tilde)\n",
        "        grad_norm = tf.sqrt(tf.reduce_sum(tf.square(d_grad), axis=[1, 2, 3]))\n",
        "\n",
        "        return tf.reduce_mean(tf.square(tf.maximum(0.0, grad_norm - 1.0)))\n",
        "\n",
        "    @tf.function\n",
        "    def train_discriminator(self, x_batch):\n",
        "        with tf.GradientTape() as D_tape:\n",
        "            # sample data\n",
        "            x_gen = self.G(tf.random.uniform([x_batch.shape[0], self.noise_dim]), training=True)\n",
        "\n",
        "            # scoring with the discriminator\n",
        "            y_real = self.D(x_batch, training=True)\n",
        "            y_gen = self.D(x_gen, training=True)\n",
        "\n",
        "            # compute the objective\n",
        "            d_obj = tf.math.reduce_mean(y_gen) - tf.math.reduce_mean(y_real)\n",
        "            d_obj_pen = d_obj + self.grad_penalty * self.lipschitz_penalty(x_batch, x_gen)\n",
        "            # update the discriminator\n",
        "            d_grad = D_tape.gradient(d_obj_pen, self.D.trainable_variables)\n",
        "            self.D_optimizer.apply_gradients(zip(d_grad, self.D.trainable_variables))\n",
        "\n",
        "        return d_obj\n",
        "\n",
        "    @tf.function\n",
        "    def train_generator(self, x_batch_size):\n",
        "        with tf.GradientTape() as G_tape:\n",
        "            x_gen = self.G(tf.random.uniform([x_batch_size, self.noise_dim]), training=True)\n",
        "            y_gen = self.D(x_gen, training=True)\n",
        "\n",
        "            # compute the objective\n",
        "            g_obj = -tf.math.reduce_mean(y_gen)\n",
        "            # update the generator\n",
        "            g_grad = G_tape.gradient(g_obj, self.G.trainable_variables)\n",
        "            self.G_optimizer.apply_gradients(zip(g_grad, self.G.trainable_variables))\n",
        "\n",
        "        return g_obj\n",
        "\n",
        "    def train(self):\n",
        "        vis_seed = None\n",
        "        if self.visualize:\n",
        "            # Seed for checking training progress\n",
        "            vis_seed = tf.random.uniform([16, self.noise_dim])\n",
        "\n",
        "        # Record current time and start training\n",
        "        print(\"Training...\")\n",
        "        ts_start = tf.timestamp()\n",
        "        for t in range(self.total_epoch):\n",
        "            batch_id = 0\n",
        "            for b in self.x_train:\n",
        "                for k in range(self.critic_step):\n",
        "                    self.d_obj[batch_id, t, k] = self.train_discriminator(b)\n",
        "                self.g_obj[batch_id, t] = self.train_generator(b.shape[0])\n",
        "                batch_id += 1\n",
        "            \n",
        "            checkpoint_dir = self.out_path + '/training_checkpoints/'\n",
        "            checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "            checkpoint = tf.train.Checkpoint(generator_optimizer=self.G_optimizer,\n",
        "                                            discriminator_optimizer=self.D_optimizer,\n",
        "                                            generator=self.G,\n",
        "                                            discriminator=self.D)\n",
        "\n",
        "            # Save the model every epoch\n",
        "            if (t + 1) % 1 == 0:\n",
        "              checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "            # Print time\n",
        "            print(\"Time used for epoch {} are {:0.2f} seconds.\".format(t + 1, tf.timestamp() - ts_start))\n",
        "\n",
        "            # Check current generator\n",
        "            if self.visualize:\n",
        "                vis_gen = self.G(vis_seed, training=False)\n",
        "                fig = plt.figure(figsize=(4, 4))\n",
        "                plt.suptitle('Epoch: {:03d}'.format(t + 1))\n",
        "                for i in range(vis_gen.shape[0]):\n",
        "                    plt.subplot(4, 4, i + 1)\n",
        "                    if self.data_shape[3] == 1:\n",
        "                        plt.imshow(vis_gen[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "                    else:\n",
        "                        plt.imshow((vis_gen[i, :, :] + 1) / 2)\n",
        "                    plt.axis('off')\n",
        "                make_dir_if_not_exists(os.path.join(self.out_path, \"WGAN_LP_{}_Epoch_{:03d}.png\".format(self.dataset, t + 1)))\n",
        "                plt.savefig(os.path.join(self.out_path, \"WGAN_LP_{}_Epoch_{:03d}.png\".format(self.dataset, t + 1)))\n",
        "                plt.clf()\n",
        "                plt.close(fig)\n",
        "        print(\"Done! {:0.2f} seconds have passed.\".format(tf.timestamp() - ts_start))\n",
        "    \n",
        "    def restore_ckpt(self):\n",
        "      checkpoint_dir = checkpoint_dir = self.out_path + '/training_checkpoints/'\n",
        "      checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "    def display_image(self,epoch_no):\n",
        "      return PIL.Image.open(os.path.join(self.out_path, \"WGAN_LP_{}_Epoch_{:03d}.png\".format(self.dataset,epoch_no)))\n",
        "    \n",
        "    def display_image(self,epoch_no,example=False):\n",
        "      if example:\n",
        "        return PIL.Image.open(os.path.join(self.out_path,\"WGANLP_{}_Example.png\".format(self.dataset)))\n",
        "      else:\n",
        "        return PIL.Image.open(os.path.join(self.out_path, \"WGAN_LP_{}_Epoch_{:03d}.png\".format(self.dataset,epoch_no)))\n",
        "\n",
        "    def make_gif(self,param):\n",
        "      anim_file = \"{}_{}_totalepoch{}.gif\".format(model_param.get(\"dataset\"),model_param.get(\"model\"),model_param.get(\"total_epoch\"))\n",
        "\n",
        "      os.chdir(model_param.get(\"output\"))\n",
        "      filenames = glob.glob('WGAN_LP_{}_Epoch_*.png'.format(model_param.get(\"dataset\")))\n",
        "      filenames = sorted(filenames)\n",
        "      image=[]\n",
        "      for i,filename in enumerate(filenames):\n",
        "        image.append(imageio.imread(filename))\n",
        "        imageio.mimsave(\"./\"+anim_file,iter(image))\n",
        "          \n",
        "      import IPython\n",
        "      if IPython.version_info > (6,2,0,''):\n",
        "        display.Image(filename=anim_file)\n",
        "\n",
        "\n",
        "def _get_params_dir():\n",
        "  from pathlib import Path\n",
        "  return os.path.join(Path.home(), 'GanvWGAN')\n",
        "\n",
        "\n",
        "def make_dir_if_not_exists(save_file):\n",
        "  \"\"\"Utility function for creating necessary dictories for a specified filename.\n",
        "  Parameters\n",
        "  ----------\n",
        "  save_file : :obj:`str`\n",
        "      absolute path of save file\n",
        "  \"\"\"\n",
        "  save_dir = os.path.dirname(save_file)\n",
        "  if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "\n",
        "def take_input():\n",
        "  \"\"\"Set up information about the model\"\"\"\n",
        "\n",
        "  model_param = {\n",
        "      'model': ['Model to fit GAN/WGANLP: ', str],\n",
        "      'dataset': ['Dataset to use MNIST/CIFAR10: ', str],\n",
        "      'output': ['Directory to save the output: ', str],\n",
        "      'batch_size': ['Size of the batch used in training: ', int],\n",
        "      'noise_dim': ['Dimension of the latent noise: ', int],\n",
        "      'total_epoch': ['Number of training epochs: ', int],\n",
        "      'critic_step': ['The number of steps to apply to the discriminator: ', int],\n",
        "      'visualize': ['Do you want to visualize the training process and the resulting models? (True/False): ', bool],\n",
        "      'gif': ['Do you want to mae a gif of the images generated at each epoch? (True/False): ', bool],\n",
        "\n",
        "      # setup the discriminator optimizer\n",
        "      'learning_rate_d': ['The learning rates of ADAM (discriminator):', float],\n",
        "      'beta_1_d': ['The exponential decay rates for the 1st moment estimates in ADAM (discriminator): ', float],\n",
        "      'beta_2_d': ['The exponential decay rates for the 2nd moment estimates in ADAM (discriminator): ', float],\n",
        "      'epsilon_d': ['Small constants for numerical stability of ADAM (discriminator): ', float],\n",
        "      'amsgrad_d': ['Logical indicating whether to use the AMSGrad variant of ADAM (discriminator): ', bool],\n",
        "\n",
        "      # setup the generator optimizer\n",
        "      'learning_rate_g': ['The learning rates of ADAM (discriminator):', float],\n",
        "      'beta_1_g': ['The exponential decay rates for the 1st moment estimates in ADAM (discriminator): ', float],\n",
        "      'beta_2_g': ['The exponential decay rates for the 2nd moment estimates in ADAM (discriminator): ', float],\n",
        "      'epsilon_g': ['Small constants for numerical stability of ADAM (discriminator): ', float],\n",
        "      'amsgrad_g': ['Logical indicating whether to use the AMSGrad variant of ADAM (discriminator): ', bool]\n",
        "  }\n",
        "\n",
        "  print('Please enter the following information:')\n",
        "  for key, val in model_param.items():\n",
        "      if val[1] == bool:\n",
        "          model_param[key] = True if input(val[0]).lower() == 'true' else False\n",
        "      else:\n",
        "          model_param[key] = val[1](input(val[0]))\n",
        "\n",
        "  if model_param.get(\"model\") == \"WGANLP\":\n",
        "      params_WGANLP = {\n",
        "          'grad_penalty': ['Penalty controlling the strength of the gradient regularization in WGAN-LP:' , float],\n",
        "          'perturb_factor': ['Factor controlling the standard deviation of perturbation for generating samples to compute the gradient penalty in WGAN-LP:', float],\n",
        "      }\n",
        "      print('Please enter the following information for WGAN-LP:')\n",
        "      for key, val in params_WGANLP.items():\n",
        "          if val[1] == bool:\n",
        "              params_WGANLP[key] = True if input(val[0]).lower() == 'true' else False\n",
        "          else:\n",
        "              params_WGANLP[key] = val[1](input(val[0]))\n",
        "\n",
        "  params_dir = _get_params_dir()\n",
        "  if not os.path.exists(params_dir):\n",
        "      os.makedirs(params_dir)\n",
        "  params_file = os.path.join(\n",
        "      params_dir, str('%s_%s_params.json' % (model_param['dataset'], model_param['model'])))\n",
        "  with open(params_file, 'w') as f:\n",
        "      json.dump(model_param, f, sort_keys=False, indent=4)\n",
        "\n",
        "  # print('Dataset params are now stored in %s' % params_file)\n",
        "\n",
        "  return model_param"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vNKkbVFFnCH"
      },
      "source": [
        "os.chdir('/content/drive/My Drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUykF4whFs0u"
      },
      "source": [
        "model_param_WGAN100drop = {'amsgrad_d': False,\n",
        " 'amsgrad_g': False,\n",
        " 'batch_size': 128,\n",
        " 'beta_1_d': 0.2,\n",
        " 'beta_1_g': 0.5,\n",
        " 'beta_2_d': 0.999,\n",
        " 'beta_2_g': 0.999,\n",
        " 'critic_step': 1,\n",
        " 'dataset': 'MNIST',\n",
        " 'epsilon_d': 1e-07,\n",
        " 'epsilon_g': 1e-07,\n",
        " 'learning_rate_d': 0.0001,\n",
        " 'learning_rate_g': 0.0001,\n",
        " 'model': 'WGANLP',\n",
        " 'noise_dim': 128,\n",
        " 'output': '/content/drive/My Drive/WGAN100',\n",
        " 'total_epoch': 100,\n",
        " 'visualize': True,\n",
        "  'gif': True}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erc5CfqNGJTs"
      },
      "source": [
        "import random\n",
        "random.seed(0)\n",
        "run(False,model_param_WGAN100drop)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}